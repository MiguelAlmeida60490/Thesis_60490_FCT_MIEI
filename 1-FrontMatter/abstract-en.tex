%!TEX root = ../template.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% abstract-en.tex
%% NOVA thesis document file
%%
%% Abstract in English
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\typeout{NT FILE abstract-en.tex}%

Code review is a critical stage in the software development lifecycle, ensuring code quality, consistency, and adherence to best practices. In complex enterprise environments, characterized by heterogeneous codebases, multiple technologies, and high volumes of changes, manual code review becomes time-consuming, prone to inconsistencies, and dependent on the expertise of reviewers.

The advent of large language models (\glspl{LLM}) offers opportunities to automate part of this process by providing contextualized feedback and reducing repetitive tasks. However, the practical use of \glspl{LLM} in enterprise-specific contexts faces challenges, including limited knowledge of internal standards, variability in comment quality, false positives, and integration into existing workflows.

This dissertation proposes the development of an \gls{LLM}-based code review support system for Processware, capable of automatically analyzing code changes, generating comments aligned with internal best practices, and integrating seamlessly into the development workflow. The solution is modular, comprising components for change extraction and preprocessing, analysis orchestration, \gls{LLM} engine, comment validation, organizational knowledge base, and results publication. The system aims to enhance efficiency, consistency, and quality in code reviews while reducing manual effort and promoting the standardization of internal practices.

Keywords: code review, artificial intelligence, large language models, automation, internal best practices.
